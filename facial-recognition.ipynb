{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước tiên chúng ta sẽ nạp bộ dữ liệu để train từ folder train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 457 images belonging to 22 classes.\n",
      "Found 457 images belonging to 22 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Hưng': 0,\n",
       " 'Hồng': 1,\n",
       " 'Nhân': 2,\n",
       " 'Thịnh': 3,\n",
       " 'Tài': 4,\n",
       " 'face1': 5,\n",
       " 'face10': 6,\n",
       " 'face11': 7,\n",
       " 'face12': 8,\n",
       " 'face13': 9,\n",
       " 'face14': 10,\n",
       " 'face15': 11,\n",
       " 'face16': 12,\n",
       " 'face2': 13,\n",
       " 'face3': 14,\n",
       " 'face4': 15,\n",
       " 'face5': 16,\n",
       " 'face6': 17,\n",
       " 'face7': 18,\n",
       " 'face8': 19,\n",
       " 'face9': 20,\n",
       " 'unknown': 21}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    " \n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    " \n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath='./train'\n",
    " \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Understand more about ImageDataGenerator at below link\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    " \n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    " \n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()\n",
    " \n",
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        color_mode = \"grayscale\",\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    " \n",
    " \n",
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        color_mode = \"grayscale\",\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    " \n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn dưới là đánh dấu từng nhãn theo dictionary rồi lưu vào file cho trường hợp sau này muốn test model không cần chạy lại đoạn ở trên cũng như ở dưới đây"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  Hưng\n",
      "1  :  Hồng\n",
      "2  :  Nhân\n",
      "3  :  Thịnh\n",
      "4  :  Tài\n",
      "5  :  face1\n",
      "6  :  face10\n",
      "7  :  face11\n",
      "8  :  face12\n",
      "9  :  face13\n",
      "10  :  face14\n",
      "11  :  face15\n",
      "12  :  face16\n",
      "13  :  face2\n",
      "14  :  face3\n",
      "15  :  face4\n",
      "16  :  face5\n",
      "17  :  face6\n",
      "18  :  face7\n",
      "19  :  face8\n",
      "20  :  face9\n",
      "21  :  unknown\n",
      "Mapping of Face and its ID {0: 'Hưng', 1: 'Hồng', 2: 'Nhân', 3: 'Thịnh', 4: 'Tài', 5: 'face1', 6: 'face10', 7: 'face11', 8: 'face12', 9: 'face13', 10: 'face14', 11: 'face15', 12: 'face16', 13: 'face2', 14: 'face3', 15: 'face4', 16: 'face5', 17: 'face6', 18: 'face7', 19: 'face8', 20: 'face9', 21: 'unknown'}\n",
      "\n",
      " The Number of output neurons:  22\n"
     ]
    }
   ],
   "source": [
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    " \n",
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    print(faceValue,' : ',faceName)\n",
    "    ResultMap[faceValue]=faceName\n",
    " \n",
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)\n",
    " \n",
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo model CNN gồm 4 lớp hidden layers , 22 đầu ra , activation là Leaky Relu , áp dụng Batch Normalization và \n",
    "dropout để giảm over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense,LeakyReLU,BatchNormalization,Dropout\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(3, 3), strides=(1, 1), input_shape=(64,64,1), activation= LeakyReLU(alpha=0.1)))\n",
    "classifier.add(BatchNormalization())\n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(128, kernel_size=(3, 3), strides=(1, 1),  activation= LeakyReLU(alpha=0.2)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    " \n",
    "classifier.add(Convolution2D(256, kernel_size=(3, 3), strides=(1, 1),  activation= LeakyReLU(alpha=0.2)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "classifier.add(Convolution2D(512, kernel_size=(3, 3), strides=(1, 1),  activation= LeakyReLU(alpha=0.3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn code ở dưới chỉ chạy sau khi đã train xong model trước đó và muốn thử kết quả , không cần chạy lại những đoạn code ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "classifier = load_model('./train-weight/21out_v4.h5') # sửa tên file .h5 cũng như đường dẫn nếu thay bằng bộ W khác\n",
    "infile = open('./ResultsMap.pkl','rb')\n",
    "ResultMap = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "457/457 [==============================] - 57s 121ms/step - loss: 2.6914 - accuracy: 0.2035 - val_loss: 2.4733 - val_accuracy: 0.2451\n",
      "Epoch 2/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 2.3339 - accuracy: 0.2451 - val_loss: 2.4079 - val_accuracy: 0.2319\n",
      "Epoch 3/80\n",
      "457/457 [==============================] - 58s 127ms/step - loss: 2.2205 - accuracy: 0.3129 - val_loss: 2.1101 - val_accuracy: 0.3151\n",
      "Epoch 4/80\n",
      "457/457 [==============================] - 63s 139ms/step - loss: 2.1875 - accuracy: 0.3129 - val_loss: 2.1204 - val_accuracy: 0.3479\n",
      "Epoch 5/80\n",
      "457/457 [==============================] - 57s 124ms/step - loss: 2.0272 - accuracy: 0.3414 - val_loss: 1.9995 - val_accuracy: 0.3632\n",
      "Epoch 6/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 2.0174 - accuracy: 0.3523 - val_loss: 1.9416 - val_accuracy: 0.3939\n",
      "Epoch 7/80\n",
      "457/457 [==============================] - 60s 131ms/step - loss: 1.9673 - accuracy: 0.3523 - val_loss: 2.0356 - val_accuracy: 0.3676\n",
      "Epoch 8/80\n",
      "457/457 [==============================] - 59s 130ms/step - loss: 1.9730 - accuracy: 0.3676 - val_loss: 2.1807 - val_accuracy: 0.3348\n",
      "Epoch 9/80\n",
      "457/457 [==============================] - 60s 132ms/step - loss: 1.9703 - accuracy: 0.3545 - val_loss: 2.1384 - val_accuracy: 0.3392\n",
      "Epoch 10/80\n",
      "457/457 [==============================] - 58s 126ms/step - loss: 1.9090 - accuracy: 0.4092 - val_loss: 1.8949 - val_accuracy: 0.4486\n",
      "Epoch 11/80\n",
      "457/457 [==============================] - 62s 136ms/step - loss: 1.8408 - accuracy: 0.4333 - val_loss: 1.8336 - val_accuracy: 0.4376\n",
      "Epoch 12/80\n",
      "457/457 [==============================] - 57s 126ms/step - loss: 1.7937 - accuracy: 0.4311 - val_loss: 2.0801 - val_accuracy: 0.4092\n",
      "Epoch 13/80\n",
      "457/457 [==============================] - 68s 150ms/step - loss: 1.7766 - accuracy: 0.4442 - val_loss: 1.8499 - val_accuracy: 0.4617\n",
      "Epoch 14/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 1.7232 - accuracy: 0.4486 - val_loss: 2.0761 - val_accuracy: 0.3632\n",
      "Epoch 15/80\n",
      "457/457 [==============================] - 54s 118ms/step - loss: 1.6304 - accuracy: 0.4836 - val_loss: 1.6630 - val_accuracy: 0.5011\n",
      "Epoch 16/80\n",
      "457/457 [==============================] - 55s 120ms/step - loss: 1.6194 - accuracy: 0.4945 - val_loss: 1.6243 - val_accuracy: 0.4770\n",
      "Epoch 17/80\n",
      "457/457 [==============================] - 64s 139ms/step - loss: 1.6275 - accuracy: 0.4967 - val_loss: 1.9528 - val_accuracy: 0.4223\n",
      "Epoch 18/80\n",
      "457/457 [==============================] - 66s 145ms/step - loss: 1.6638 - accuracy: 0.4880 - val_loss: 1.7934 - val_accuracy: 0.4573\n",
      "Epoch 19/80\n",
      "457/457 [==============================] - 58s 128ms/step - loss: 1.6166 - accuracy: 0.4902 - val_loss: 1.6968 - val_accuracy: 0.4486\n",
      "Epoch 20/80\n",
      "457/457 [==============================] - 58s 127ms/step - loss: 1.5534 - accuracy: 0.5230 - val_loss: 1.5136 - val_accuracy: 0.5208\n",
      "Epoch 21/80\n",
      "457/457 [==============================] - 63s 137ms/step - loss: 1.5128 - accuracy: 0.5295 - val_loss: 1.4537 - val_accuracy: 0.5449\n",
      "Epoch 22/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 1.5223 - accuracy: 0.5164 - val_loss: 1.5148 - val_accuracy: 0.5274\n",
      "Epoch 23/80\n",
      "457/457 [==============================] - 59s 130ms/step - loss: 1.5344 - accuracy: 0.5186 - val_loss: 1.5702 - val_accuracy: 0.4967\n",
      "Epoch 24/80\n",
      "457/457 [==============================] - 58s 126ms/step - loss: 1.4501 - accuracy: 0.5383 - val_loss: 1.6676 - val_accuracy: 0.5142\n",
      "Epoch 25/80\n",
      "457/457 [==============================] - 60s 132ms/step - loss: 1.4233 - accuracy: 0.5449 - val_loss: 1.4189 - val_accuracy: 0.5470\n",
      "Epoch 26/80\n",
      "457/457 [==============================] - 60s 132ms/step - loss: 1.4174 - accuracy: 0.5449 - val_loss: 1.5338 - val_accuracy: 0.5098\n",
      "Epoch 27/80\n",
      "457/457 [==============================] - 61s 134ms/step - loss: 1.3781 - accuracy: 0.5580 - val_loss: 1.4284 - val_accuracy: 0.5405\n",
      "Epoch 28/80\n",
      "457/457 [==============================] - 52s 114ms/step - loss: 1.3068 - accuracy: 0.5777 - val_loss: 1.4087 - val_accuracy: 0.5492\n",
      "Epoch 29/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 1.3833 - accuracy: 0.5536 - val_loss: 3.6028 - val_accuracy: 0.2867\n",
      "Epoch 30/80\n",
      "457/457 [==============================] - 56s 122ms/step - loss: 1.4168 - accuracy: 0.5492 - val_loss: 1.4601 - val_accuracy: 0.5427\n",
      "Epoch 31/80\n",
      "457/457 [==============================] - 52s 114ms/step - loss: 1.4156 - accuracy: 0.5427 - val_loss: 1.6337 - val_accuracy: 0.4902\n",
      "Epoch 32/80\n",
      "457/457 [==============================] - 53s 117ms/step - loss: 1.3035 - accuracy: 0.5799 - val_loss: 1.4112 - val_accuracy: 0.5580\n",
      "Epoch 33/80\n",
      "457/457 [==============================] - 52s 114ms/step - loss: 1.3331 - accuracy: 0.5733 - val_loss: 1.3854 - val_accuracy: 0.5624\n",
      "Epoch 34/80\n",
      "457/457 [==============================] - 367s 806ms/step - loss: 1.3782 - accuracy: 0.5492 - val_loss: 1.6074 - val_accuracy: 0.4967\n",
      "Epoch 35/80\n",
      "457/457 [==============================] - 47s 102ms/step - loss: 1.3544 - accuracy: 0.5602 - val_loss: 1.4693 - val_accuracy: 0.5361\n",
      "Epoch 36/80\n",
      "457/457 [==============================] - 51s 111ms/step - loss: 1.3830 - accuracy: 0.5580 - val_loss: 1.3535 - val_accuracy: 0.5580\n",
      "Epoch 37/80\n",
      "457/457 [==============================] - 51s 111ms/step - loss: 1.3579 - accuracy: 0.5602 - val_loss: 1.5349 - val_accuracy: 0.5186\n",
      "Epoch 38/80\n",
      "457/457 [==============================] - 52s 113ms/step - loss: 1.2653 - accuracy: 0.5799 - val_loss: 1.4587 - val_accuracy: 0.5514\n",
      "Epoch 39/80\n",
      "457/457 [==============================] - 52s 114ms/step - loss: 1.3275 - accuracy: 0.5733 - val_loss: 1.6042 - val_accuracy: 0.5077\n",
      "Epoch 40/80\n",
      "457/457 [==============================] - 54s 119ms/step - loss: 1.2666 - accuracy: 0.5886 - val_loss: 1.6841 - val_accuracy: 0.5405\n",
      "Epoch 41/80\n",
      "457/457 [==============================] - 51s 113ms/step - loss: 1.3019 - accuracy: 0.5755 - val_loss: 1.6720 - val_accuracy: 0.4770\n",
      "Epoch 42/80\n",
      "457/457 [==============================] - 54s 118ms/step - loss: 1.2589 - accuracy: 0.5821 - val_loss: 1.3503 - val_accuracy: 0.5646\n",
      "Epoch 43/80\n",
      "457/457 [==============================] - 53s 116ms/step - loss: 1.2990 - accuracy: 0.5733 - val_loss: 1.3115 - val_accuracy: 0.5667\n",
      "Epoch 44/80\n",
      "457/457 [==============================] - 52s 114ms/step - loss: 1.2789 - accuracy: 0.5799 - val_loss: 1.2895 - val_accuracy: 0.5558\n",
      "Epoch 45/80\n",
      "457/457 [==============================] - 51s 112ms/step - loss: 1.1555 - accuracy: 0.6039 - val_loss: 1.4821 - val_accuracy: 0.5470\n",
      "Epoch 46/80\n",
      "457/457 [==============================] - 55s 120ms/step - loss: 1.2165 - accuracy: 0.6083 - val_loss: 1.4634 - val_accuracy: 0.5033\n",
      "Epoch 47/80\n",
      "457/457 [==============================] - 588s 1s/step - loss: 1.2137 - accuracy: 0.6039 - val_loss: 1.3099 - val_accuracy: 0.5536\n",
      "Epoch 48/80\n",
      "457/457 [==============================] - 55s 120ms/step - loss: 1.1955 - accuracy: 0.6018 - val_loss: 1.2250 - val_accuracy: 0.6083\n",
      "Epoch 49/80\n",
      "457/457 [==============================] - 51s 111ms/step - loss: 1.1621 - accuracy: 0.6149 - val_loss: 1.3118 - val_accuracy: 0.5689\n",
      "Epoch 50/80\n",
      "457/457 [==============================] - 53s 115ms/step - loss: 1.1427 - accuracy: 0.6236 - val_loss: 1.1702 - val_accuracy: 0.6105\n",
      "Epoch 51/80\n",
      "457/457 [==============================] - 57s 125ms/step - loss: 1.1459 - accuracy: 0.6105 - val_loss: 1.2263 - val_accuracy: 0.6127\n",
      "Epoch 52/80\n",
      "457/457 [==============================] - 51s 112ms/step - loss: 1.1569 - accuracy: 0.6214 - val_loss: 1.3743 - val_accuracy: 0.5427\n",
      "Epoch 53/80\n",
      "457/457 [==============================] - 51s 112ms/step - loss: 1.2338 - accuracy: 0.6018 - val_loss: 1.1731 - val_accuracy: 0.5996\n",
      "Epoch 54/80\n",
      "457/457 [==============================] - 51s 111ms/step - loss: 1.0062 - accuracy: 0.6477 - val_loss: 1.5047 - val_accuracy: 0.5755\n",
      "Epoch 55/80\n",
      "457/457 [==============================] - 53s 116ms/step - loss: 1.0237 - accuracy: 0.6565 - val_loss: 1.0134 - val_accuracy: 0.6740\n",
      "Epoch 56/80\n",
      "457/457 [==============================] - 55s 121ms/step - loss: 1.1171 - accuracy: 0.6258 - val_loss: 1.4265 - val_accuracy: 0.5536\n",
      "Epoch 57/80\n",
      "457/457 [==============================] - 57s 124ms/step - loss: 1.1124 - accuracy: 0.6324 - val_loss: 1.3556 - val_accuracy: 0.5470\n",
      "Epoch 58/80\n",
      "457/457 [==============================] - 60s 132ms/step - loss: 1.0359 - accuracy: 0.6346 - val_loss: 1.1953 - val_accuracy: 0.5996\n",
      "Epoch 59/80\n",
      "457/457 [==============================] - 57s 124ms/step - loss: 1.0375 - accuracy: 0.6280 - val_loss: 1.2676 - val_accuracy: 0.6083\n",
      "Epoch 60/80\n",
      "457/457 [==============================] - 54s 119ms/step - loss: 1.0599 - accuracy: 0.6411 - val_loss: 1.1598 - val_accuracy: 0.6149\n",
      "Epoch 61/80\n",
      "457/457 [==============================] - 62s 136ms/step - loss: 1.0108 - accuracy: 0.6477 - val_loss: 1.3114 - val_accuracy: 0.5952\n",
      "Epoch 62/80\n",
      "457/457 [==============================] - 58s 127ms/step - loss: 0.9581 - accuracy: 0.6630 - val_loss: 1.2686 - val_accuracy: 0.5974\n",
      "Epoch 63/80\n",
      "457/457 [==============================] - 58s 128ms/step - loss: 1.0219 - accuracy: 0.6499 - val_loss: 1.2551 - val_accuracy: 0.6214\n",
      "Epoch 64/80\n",
      "457/457 [==============================] - 63s 138ms/step - loss: 0.9609 - accuracy: 0.6565 - val_loss: 1.3823 - val_accuracy: 0.5886\n",
      "Epoch 65/80\n",
      "457/457 [==============================] - 67s 147ms/step - loss: 0.9753 - accuracy: 0.6652 - val_loss: 1.3165 - val_accuracy: 0.6127\n",
      "Epoch 66/80\n",
      "457/457 [==============================] - 65s 142ms/step - loss: 0.9268 - accuracy: 0.6674 - val_loss: 1.1212 - val_accuracy: 0.6324\n",
      "Epoch 67/80\n",
      "457/457 [==============================] - 61s 135ms/step - loss: 0.9406 - accuracy: 0.6674 - val_loss: 1.3116 - val_accuracy: 0.6105\n",
      "Epoch 68/80\n",
      "457/457 [==============================] - 62s 135ms/step - loss: 0.9907 - accuracy: 0.6499 - val_loss: 1.5382 - val_accuracy: 0.5711\n",
      "Epoch 69/80\n",
      "457/457 [==============================] - 66s 143ms/step - loss: 0.9963 - accuracy: 0.6521 - val_loss: 1.1203 - val_accuracy: 0.6324\n",
      "Epoch 70/80\n",
      "457/457 [==============================] - 60s 132ms/step - loss: 0.9689 - accuracy: 0.6630 - val_loss: 1.2071 - val_accuracy: 0.6149\n",
      "Epoch 71/80\n",
      "457/457 [==============================] - 62s 135ms/step - loss: 0.9081 - accuracy: 0.6652 - val_loss: 0.9947 - val_accuracy: 0.6696\n",
      "Epoch 72/80\n",
      "457/457 [==============================] - 62s 135ms/step - loss: 0.9255 - accuracy: 0.6783 - val_loss: 1.0956 - val_accuracy: 0.6258\n",
      "Epoch 73/80\n",
      "457/457 [==============================] - 62s 135ms/step - loss: 0.9895 - accuracy: 0.6608 - val_loss: 1.1903 - val_accuracy: 0.6499\n",
      "Epoch 74/80\n",
      "457/457 [==============================] - 66s 144ms/step - loss: 0.9494 - accuracy: 0.6586 - val_loss: 1.3451 - val_accuracy: 0.5952\n",
      "Epoch 75/80\n",
      "457/457 [==============================] - 66s 144ms/step - loss: 1.0059 - accuracy: 0.6455 - val_loss: 1.2085 - val_accuracy: 0.6302\n",
      "Epoch 76/80\n",
      "457/457 [==============================] - 69s 150ms/step - loss: 0.9190 - accuracy: 0.6630 - val_loss: 1.2565 - val_accuracy: 0.6324\n",
      "Epoch 77/80\n",
      "457/457 [==============================] - 64s 139ms/step - loss: 0.8997 - accuracy: 0.6783 - val_loss: 1.1123 - val_accuracy: 0.6433\n",
      "Epoch 78/80\n",
      "457/457 [==============================] - 70s 153ms/step - loss: 1.0036 - accuracy: 0.6696 - val_loss: 0.9819 - val_accuracy: 0.6696\n",
      "Epoch 79/80\n",
      "457/457 [==============================] - 79s 173ms/step - loss: 0.8788 - accuracy: 0.6805 - val_loss: 1.1140 - val_accuracy: 0.6411\n",
      "Epoch 80/80\n",
      "457/457 [==============================] - 79s 172ms/step - loss: 0.9009 - accuracy: 0.6740 - val_loss: 1.0445 - val_accuracy: 0.6346\n",
      "###### Total Time Taken:  92 Minutes ######\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "classifier.fit(\n",
    "                   training_set,\n",
    "                    steps_per_epoch=training_set.n//training_set.batch_size,\n",
    "                    epochs=80,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=test_set.n//test_set.batch_size)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dự đoán model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "########################################\n",
      "Prediction is:  Hồng\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''########### Making single predictions ###########'''\n",
    "import numpy as np\n",
    "import keras.utils as image\n",
    " \n",
    "#ImagePath='./Face Images/Final Testing Images/anh-hung/hung5.jpg'\n",
    "ImagePath = './test/hong1.jpg'\n",
    "test_image=image.load_img(ImagePath,color_mode = \"grayscale\",target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result = classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    "print(np.argmax(result))\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu model (Nên đổi tên file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('./train-weight/21out_v4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pyyaml h5py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
